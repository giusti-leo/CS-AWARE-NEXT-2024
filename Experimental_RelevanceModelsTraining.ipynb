{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dfabffe","executionInfo":{"status":"ok","timestamp":1707322981319,"user_tz":-60,"elapsed":188806,"user":{"displayName":"Leonardo Giusti","userId":"10453788555175165530"}},"outputId":"29a22b21-c9f3-4bf0-b5fc-0afc6720b768"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow==2.12.0rc0\n","  Downloading tensorflow-2.12.0rc0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.8/585.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (23.5.26)\n","Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0rc0)\n","  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (1.60.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (3.9.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (0.4.23)\n","Collecting keras<2.13,>=2.12.0rc0 (from tensorflow==2.12.0rc0)\n","  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (16.0.6)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (1.16.0)\n","Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0rc0)\n","  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0rc0 (from tensorflow==2.12.0rc0)\n","  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (4.9.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0rc0) (0.35.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0rc0) (0.42.0)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0rc0) (0.2.0)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0rc0) (1.11.4)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (2.17.3)\n","Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0)\n","  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (3.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (2.1.5)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0rc0) (3.2.2)\n","Installing collected packages: tensorflow-estimator, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.15.0\n","    Uninstalling tensorflow-estimator-2.15.0:\n","      Successfully uninstalled tensorflow-estimator-2.15.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.4\n","    Uninstalling gast-0.5.4:\n","      Successfully uninstalled gast-0.5.4\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.2.0\n","    Uninstalling google-auth-oauthlib-1.2.0:\n","      Successfully uninstalled google-auth-oauthlib-1.2.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.15.1\n","    Uninstalling tensorboard-2.15.1:\n","      Successfully uninstalled tensorboard-2.15.1\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.15.0\n","    Uninstalling tensorflow-2.15.0:\n","      Successfully uninstalled tensorflow-2.15.0\n","Successfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.12.0 tensorboard-2.12.3 tensorflow-2.12.0rc0 tensorflow-estimator-2.12.0\n","Collecting numpy==1.18.5\n","  Downloading numpy-1.18.5.zip (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n","Collecting laser_encoders\n","  Downloading laser_encoders-0.0.1-py3-none-any.whl (24 kB)\n","Collecting sacremoses==0.1.0 (from laser_encoders)\n","  Downloading sacremoses-0.1.0-py3-none-any.whl (895 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.1/895.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting unicategories>=0.1.2 (from laser_encoders)\n","  Downloading unicategories-0.1.2.tar.gz (12 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from laser_encoders) (0.1.99)\n","Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from laser_encoders) (1.23.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from laser_encoders) (2.1.0+cu121)\n","Collecting fairseq>=0.12.2 (from laser_encoders)\n","  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.1.0->laser_encoders) (2023.12.25)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.1.0->laser_encoders) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.1.0->laser_encoders) (1.3.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.1.0->laser_encoders) (4.66.1)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.12.2->laser_encoders) (1.16.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.12.2->laser_encoders) (3.0.8)\n","Collecting hydra-core<1.1,>=1.0.7 (from fairseq>=0.12.2->laser_encoders)\n","  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting omegaconf<2.1 (from fairseq>=0.12.2->laser_encoders)\n","  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n","Collecting sacrebleu>=1.4.12 (from fairseq>=0.12.2->laser_encoders)\n","  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitarray (from fairseq>=0.12.2->laser_encoders)\n","  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.12.2->laser_encoders) (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (2.1.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from unicategories>=0.1.2->laser_encoders) (1.4.4)\n","Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq>=0.12.2->laser_encoders)\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq>=0.12.2->laser_encoders) (6.0.1)\n","Collecting portalocker (from sacrebleu>=1.4.12->fairseq>=0.12.2->laser_encoders)\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq>=0.12.2->laser_encoders) (0.9.0)\n","Collecting colorama (from sacrebleu>=1.4.12->fairseq>=0.12.2->laser_encoders)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq>=0.12.2->laser_encoders) (4.9.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq>=0.12.2->laser_encoders) (2.21)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->laser_encoders) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->laser_encoders) (1.3.0)\n","Building wheels for collected packages: fairseq, unicategories, antlr4-python3-runtime\n","  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291819 sha256=ba11b808dcd73341dc7e8571e29655615cb43fcaa486b153a9fa81c2e6296f13\n","  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n","  Building wheel for unicategories (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unicategories: filename=unicategories-0.1.2-py2.py3-none-any.whl size=30843 sha256=69a374b6454988202ec4a1cdc41d6e3b6bbf110ac914debe182895d168a65a4b\n","  Stored in directory: /root/.cache/pip/wheels/0b/6d/14/7135674b9daa3996f7f0d9bc1ccff5b7d50d6f1c4a16dc7d90\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=3dbc5ae2f1cfaeaeb2c08efc8c763e1242dac9a5ff4f2f40ff582c2d33c68379\n","  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n","Successfully built fairseq unicategories antlr4-python3-runtime\n","Installing collected packages: bitarray, antlr4-python3-runtime, unicategories, sacremoses, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq, laser_encoders\n","Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 laser_encoders-0.0.1 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0 sacremoses-0.1.0 unicategories-0.1.2\n"]}],"source":["#execute this cell, then press 'Restart and clear cell outputs' and execute all cells except this\n","!pip install -U tensorflow==2.12.0rc0\n","!pip install -U numpy==1.18.5\n","!pip install laser_encoders"],"id":"8dfabffe"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ed18a7d8","executionInfo":{"status":"ok","timestamp":1707323035164,"user_tz":-60,"elapsed":53864,"user":{"displayName":"Leonardo Giusti","userId":"10453788555175165530"}},"outputId":"c4dfcc1b-9822-445a-bbdd-cac885b84fca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting contractions\n","  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n","Collecting textsearch>=0.0.21 (from contractions)\n","  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n","Collecting anyascii (from textsearch>=0.0.21->contractions)\n","  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n","  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n","Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n","Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.2.2)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n","Collecting sweetviz\n","  Downloading sweetviz-2.3.1-py3-none-any.whl (15.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.5.3)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.23.5)\n","Requirement already satisfied: matplotlib>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.7.1)\n","Requirement already satisfied: tqdm>=4.43.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (4.66.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (1.11.4)\n","Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (3.1.3)\n","Requirement already satisfied: importlib-resources>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sweetviz) (6.1.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.1->sweetviz) (2.1.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (4.47.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.3->sweetviz) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->sweetviz) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->sweetviz) (1.16.0)\n","Installing collected packages: sweetviz\n","Successfully installed sweetviz-2.3.1\n","Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","!pip install contractions\n","!pip install vaderSentiment\n","!pip install sweetviz\n","!pip install textblob\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import contractions\n","import sweetviz as sv\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","import time\n","\n","#importing the Lemming function from nltk library\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import matplotlib.pyplot as plt\n","from textblob import TextBlob\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"id":"ed18a7d8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lys_0oa0xF6"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")"],"id":"5lys_0oa0xF6"},{"cell_type":"markdown","metadata":{"id":"FfTMRvQdTIdB"},"source":["# Datasets"],"id":"FfTMRvQdTIdB"},{"cell_type":"markdown","metadata":{"id":"FZ-3l09yHSgE"},"source":["## Direct Search Posts"],"id":"FZ-3l09yHSgE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8oWtZM9Fxk1"},"outputs":[],"source":["reddit_direct_posts_dir = '/content/gdrive/MyDrive/RelevanceAnalysis/Mask_RedditDirectPosts_analyzed - RedditDirectPosts_analyzed.csv'\n","\n","reddit_direct_posts = pd.read_csv(reddit_direct_posts_dir)\n","reddit_direct_posts = reddit_direct_posts.drop(['Unnamed: 0'], axis = 1)\n","\n","print(reddit_direct_posts.shape)"],"id":"j8oWtZM9Fxk1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4WbqPFwH3Dh"},"outputs":[],"source":["reddit_direct_posts.head()"],"id":"C4WbqPFwH3Dh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"39PEK2t2GPpk"},"outputs":[],"source":["reddit_direct_posts_checked = reddit_direct_posts[reddit_direct_posts.Checked == 1]\n","reddit_direct_posts_unchecked = reddit_direct_posts[reddit_direct_posts.Checked == 0]"],"id":"39PEK2t2GPpk"},{"cell_type":"markdown","metadata":{"id":"aHUkPdqxHWej"},"source":["## Filtered Search"],"id":"aHUkPdqxHWej"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xdd5QwFHFxXs"},"outputs":[],"source":["reddit_filtered_posts_dir = '/content/gdrive/MyDrive/RelevanceAnalysis/Mask_RedditFilteredPosts_analyzed - RedditFilteredPosts_analyzed.csv'\n","\n","reddit_filtered_posts = pd.read_csv(reddit_filtered_posts_dir)\n","reddit_filtered_posts = reddit_filtered_posts.drop(['Unnamed: 0'], axis = 1)\n","\n","print(reddit_filtered_posts.shape)"],"id":"Xdd5QwFHFxXs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"BgoYZ5j2Gc2-"},"outputs":[],"source":["reddit_filtered_posts_checked = reddit_filtered_posts[reddit_filtered_posts.Checked == 1]\n","reddit_filtered_posts_unchecked = reddit_filtered_posts[reddit_filtered_posts.Checked == 0]"],"id":"BgoYZ5j2Gc2-"},{"cell_type":"markdown","metadata":{"id":"quRNnuGGmgE3"},"source":["# Mix Search"],"id":"quRNnuGGmgE3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xq4V8e8gmeq5"},"outputs":[],"source":["reddit_global_results_dir = '/content/gdrive/MyDrive/RelevanceAnalysis/GlobalResults.csv'\n","\n","reddit_global_res_posts = pd.read_csv(reddit_global_results_dir)\n","reddit_global_res_posts = reddit_global_res_posts.drop(['Unnamed: 0'], axis = 1)\n","\n","print(reddit_global_res_posts.columns)"],"id":"Xq4V8e8gmeq5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jm-DWxSJoQnK"},"outputs":[],"source":["reddit_global_res_posts.head()"],"id":"Jm-DWxSJoQnK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Gi55VClmmnp"},"outputs":[],"source":["reddit_global_posts_checked = reddit_global_res_posts[reddit_global_res_posts.Checked == 1]\n","reddit_global_posts_unchecked = reddit_global_res_posts[reddit_global_res_posts.Checked == 0]"],"id":"0Gi55VClmmnp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQbH9-QRrP7v"},"outputs":[],"source":["reddit_global_posts_checked.rename(columns={'full_text' : 'content'}, inplace=True)\n","reddit_global_posts_unchecked.rename(columns={'full_text' : 'content'}, inplace=True)\n","reddit_global_posts_unchecked.head()\n","reddit_global_posts_checked.head()"],"id":"UQbH9-QRrP7v"},{"cell_type":"markdown","metadata":{"id":"785FHrAJTTKO"},"source":["## Team Analysis"],"id":"785FHrAJTTKO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZYG15oCWMC8"},"outputs":[],"source":["pip install gspread pandas oauth2client\n"],"id":"WZYG15oCWMC8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApdgWFk9VC7W"},"outputs":[],"source":["import gspread\n","from oauth2client.service_account import ServiceAccountCredentials\n","import pandas as pd"],"id":"ApdgWFk9VC7W"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkGO4DwkYXjG"},"outputs":[],"source":["from google.colab import auth\n","auth.authenticate_user()"],"id":"QkGO4DwkYXjG"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjcNpbsPTagb"},"outputs":[],"source":["reddit_posts3 = 'https://docs.google.com/spreadsheets/d/1JkmcuuKJsipjHyK9HDJ64xbGE-zIz2rB7KyloeSvnuI/edit#gid=2102769814'\n","reddit_posts2 = 'https://docs.google.com/spreadsheets/d/1NiJYJoIgAeKkYuGY338MtMmuSdjlsgisT_zDA8b_WKc/edit#gid=884260924'\n","reddit_posts1 = 'https://docs.google.com/spreadsheets/d/14bc3_VPW0I8Pcf40ZF78BcHI1AoAZ3nhdZdqRnNvBEY/edit#gid=1305219599'\n","reddit_posts4 = 'https://docs.google.com/spreadsheets/d/1hQOwgRf7jIWLmvksr1v_BmdwOJap1PrKoSmrs5-HS78/edit#gid=416251711'\n","reddit_posts5 = 'https://docs.google.com/spreadsheets/d/1wuUscml8_YykP1dC802BpcBMD2nGFxHTywaWDCTh0Tk/edit#gid=787134330'\n","reddit_posts6 = 'https://docs.google.com/spreadsheets/d/1BlDYry6Q2VueejyuUn7kncRJmPmrqKgYKO9yO_ZQtfU/edit#gid=773392604'\n","reddit_posts7 = 'https://docs.google.com/spreadsheets/d/1OZ9tqkb_dnl69SRbLfcifmqxuIf415drkeuDqYNUg9o/edit#gid=1806656778'\n","reddit_posts8 = 'https://docs.google.com/spreadsheets/d/1iaftVk5DqCsUF93712hVJke3OL_1WTziaIlDPwaaTBs/edit#gid=422591794'"],"id":"yjcNpbsPTagb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPi3wqzuarus"},"outputs":[],"source":["import os\n","\n","current_directory = os.getcwd()\n","print(current_directory)"],"id":"KPi3wqzuarus"},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYF-n9rEdA0S"},"outputs":[],"source":["nuovi_nomi_colonne = ['content', 'keyword', 'keyword type', 'Checked', 'Manual Evaluation']\n"],"id":"EYF-n9rEdA0S"},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDLWczk5YVF4"},"outputs":[],"source":["from google.auth import default\n","creds, _ = default()\n","\n","gs = gspread.authorize(creds)\n","\n","\n","# WORKSHEET 1\n","\n","worksheet1 = gs.open_by_url(reddit_posts1).sheet1\n","\n","rows1 = worksheet1.get_all_values()\n","\n","df1 = pd.DataFrame(rows1)\n","df1 = df1.iloc[1:, 1:]\n","df1.columns = nuovi_nomi_colonne\n","\n","print(df1)\n","\n","\n","# WORKSHEET 2\n","\n","worksheet2 = gs.open_by_url(reddit_posts2).sheet1\n","\n","rows2 = worksheet2.get_all_values()\n","\n","df2 = pd.DataFrame(rows2)\n","df2 = df2.iloc[1:, 1:]\n","df2.columns = nuovi_nomi_colonne\n","\n","# WORKSHEET 3\n","\n","worksheet3 = gs.open_by_url(reddit_posts3).sheet1\n","\n","rows3 = worksheet3.get_all_values()\n","\n","df3 = pd.DataFrame(rows3)\n","df3 = df3.iloc[1:, 1:]\n","df3.columns = nuovi_nomi_colonne\n","\n","print(df3)\n","\n","# WORKSHEET 4\n","\n","worksheet4 = gs.open_by_url(reddit_posts4).sheet1\n","\n","rows4 = worksheet4.get_all_values()\n","\n","df4 = pd.DataFrame(rows4)\n","df4 = df4.iloc[1:, 1:]\n","df4.columns = nuovi_nomi_colonne\n","\n","print(df4)\n","\n","# WORKSHEET 5\n","\n","worksheet5 = gs.open_by_url(reddit_posts5).sheet1\n","\n","rows5 = worksheet5.get_all_values()\n","\n","df5 = pd.DataFrame(rows5)\n","df5 = df5.iloc[1:, 1:]\n","df5.columns = nuovi_nomi_colonne\n","\n","# WORKSHEET 6\n","\n","worksheet6 = gs.open_by_url(reddit_posts6).sheet1\n","\n","rows6 = worksheet6.get_all_values()\n","\n","df6 = pd.DataFrame(rows6)\n","df6 = df6.iloc[1:, 1:]\n","df6.columns = nuovi_nomi_colonne\n","\n","# WORKSHEET 7\n","\n","worksheet7 = gs.open_by_url(reddit_posts7).sheet1\n","\n","rows7 = worksheet7.get_all_values()\n","\n","df7 = pd.DataFrame(rows5)\n","df7 = df7.iloc[1:, 1:]\n","df7.columns = nuovi_nomi_colonne\n","\n","# WORKSHEET 8\n","\n","worksheet8 = gs.open_by_url(reddit_posts8).sheet1\n","\n","rows8 = worksheet8.get_all_values()\n","\n","df8 = pd.DataFrame(rows8)\n","df8 = df8.iloc[1:, 1:]\n","df8.columns = nuovi_nomi_colonne\n","\n","df_reddit_collection = pd.concat([df3, df8, df7, df6, df5, df4, df2, df1 ]).reset_index(drop=True)\n","\n","df_reddit_collection"],"id":"TDLWczk5YVF4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"AkO5vDKujGUe"},"outputs":[],"source":["df_reddit_collection['Checked'] = pd.to_numeric(df_reddit_collection['Checked'], errors='coerce')\n","df_reddit_collection['Manual Evaluation'] = pd.to_numeric(df_reddit_collection['Manual Evaluation'], errors='coerce')\n","df_reddit_collection['Checked'] = df_reddit_collection['Checked'].fillna(0).astype('int64')\n","df_reddit_collection['Manual Evaluation'] = df_reddit_collection['Manual Evaluation'].fillna(0).astype('int64')"],"id":"AkO5vDKujGUe"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZv2aTAThtEW"},"outputs":[],"source":["df_checked_reddit_collection = df_reddit_collection [df_reddit_collection['Checked'] == 1]\n","df_unchecked_reddit_collection = df_reddit_collection[df_reddit_collection['Checked'] == 0]\n","\n","print(f'Checked : {len(df_checked_reddit_collection)}')\n","print(f'Unchecked : {len(df_unchecked_reddit_collection)}')\n","print(f'Total : {len(df_unchecked_reddit_collection) + len(df_checked_reddit_collection)}')\n"],"id":"NZv2aTAThtEW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6GBQSEUhk3O"},"outputs":[],"source":["##"],"id":"a6GBQSEUhk3O"},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQ-hfnqITSYj"},"outputs":[],"source":[],"id":"IQ-hfnqITSYj"},{"cell_type":"markdown","metadata":{"id":"eKyBiZwsl2ao"},"source":["# Union\n"],"id":"eKyBiZwsl2ao"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MC-sXtkVHgZm"},"outputs":[],"source":["reddit_posts_checked = pd.concat([ df_checked_reddit_collection[['content', 'Manual Evaluation']], reddit_filtered_posts_checked[[ 'content', 'Manual Evaluation' ]], reddit_direct_posts_checked[[ 'content', 'Manual Evaluation' ]], reddit_global_posts_checked[[ 'content', 'Manual Evaluation' ]]])\n","\n","reddit_posts_checked.head()"],"id":"MC-sXtkVHgZm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTSEL75QHgg2"},"outputs":[],"source":["reddit_posts_unchecked = pd.concat([df_unchecked_reddit_collection[['content', 'Manual Evaluation']], reddit_filtered_posts_unchecked[[ 'content', 'Manual Evaluation' ]], reddit_direct_posts_unchecked[[ 'content', 'Manual Evaluation' ]], reddit_global_posts_unchecked[[ 'content', 'Manual Evaluation' ]]])\n","\n","reddit_posts_unchecked.head()"],"id":"WTSEL75QHgg2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8lHyiUPfJV1m"},"outputs":[],"source":["reddit_posts_checked.rename(columns={  'Manual Evaluation': 'Related', 'content' : 'full_text'}, inplace=True)\n","reddit_posts_unchecked.rename(columns={'Manual Evaluation': 'Related', 'content' : 'full_text'}, inplace=True)"],"id":"8lHyiUPfJV1m"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xS0aQfkGHgWE"},"outputs":[],"source":["reddit_posts_checked = reddit_posts_checked.drop_duplicates().reset_index(drop = True)\n","reddit_posts_checked"],"id":"xS0aQfkGHgWE"},{"cell_type":"code","execution_count":null,"metadata":{"id":"35Ww3jSlHgTb"},"outputs":[],"source":["reddit_posts_unchecked = reddit_posts_unchecked.drop_duplicates().reset_index(drop = True)\n","reddit_posts_unchecked"],"id":"35Ww3jSlHgTb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkuYCM4gHgQh"},"outputs":[],"source":[],"id":"hkuYCM4gHgQh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmiagemSHgNs"},"outputs":[],"source":[],"id":"YmiagemSHgNs"},{"cell_type":"markdown","metadata":{"id":"eb303f13"},"source":["tokenize + embeddings"],"id":"eb303f13"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0592f118"},"outputs":[],"source":["# pip install --upgrade tensorflow"],"id":"0592f118"},{"cell_type":"code","execution_count":null,"metadata":{"id":"59d57f92"},"outputs":[],"source":["!pip install keras-tuner\n","!pip install --upgrade paramiko cryptography\n"],"id":"59d57f92"},{"cell_type":"code","execution_count":null,"metadata":{"id":"84415c2a"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import json\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","\n","import tensorflow as tf\n","tfk = tf.keras\n","tfkl = tf.keras.layers"],"id":"84415c2a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"56207d28"},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","#from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.preprocessing import sequence, text\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn import metrics\n","from tensorflow.keras.initializers import Constant\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import *\n","from tensorflow.keras import backend as K\n","from sklearn import preprocessing\n","\n","import nltk # NLP\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","import re\n","\n","# visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","%matplotlib inline\n","from plotly import graph_objs as go\n","import plotly.express as px\n","import plotly.figure_factory as ff\n","\n","#Tuner\n","from keras_tuner.tuners import RandomSearch\n"],"id":"56207d28"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e7964c5"},"outputs":[],"source":["# seed initialization\n","seed = 42\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"],"id":"1e7964c5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fa76593"},"outputs":[],"source":["def plot_history(history):\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'validation'], loc='upper left')\n","    plt.show()\n","\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'validation'], loc='upper left')\n","    plt.show()"],"id":"3fa76593"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kstVeO-8LqVK"},"outputs":[],"source":["df = pd.concat([reddit_posts_checked])\n","df = df.drop_duplicates(subset=['full_text']).reset_index(drop = True)\n","(df.head())"],"id":"kstVeO-8LqVK"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f875d89f"},"outputs":[],"source":["# check the distribution of the labels\n","plt.figure(figsize=(30,10))\n","plt.title('Percentage of Labels', fontsize=20)\n","df.Related.value_counts().plot(kind='pie', wedgeprops=dict(width=.7), autopct='%1.0f%%', startangle= -20,  textprops={'fontsize': 15})"],"id":"f875d89f"},{"cell_type":"markdown","metadata":{"id":"i9clL5CBf7S9"},"source":["## PREPROCESSING\n","#### Remove links and punctuation"],"id":"i9clL5CBf7S9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7aUVYjnAf7-O"},"outputs":[],"source":["import re\n","\n","def preprocessing(text):\n","    # Rimuovi i link\n","    text = re.sub(r'http\\S+', ' ', text)\n","    # Rimuovi la punteggiatura\n","    text = re.sub(r'[^\\w\\s]', ' ', text)\n","    return text\n","\n","text = \"Hey, this link https://www.example.com !?\"\n","new_text = preprocessing(text)\n","\n","new_text"],"id":"7aUVYjnAf7-O"},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNTZhaUogUIE"},"outputs":[],"source":["df['original_text'] = df['full_text']\n","\n","df['full_text'] = df['full_text'].apply(preprocessing)\n","df['full_text']"],"id":"GNTZhaUogUIE"},{"cell_type":"markdown","metadata":{"id":"6774a698"},"source":["LASER"],"id":"6774a698"},{"cell_type":"code","execution_count":null,"metadata":{"id":"71c6f61a"},"outputs":[],"source":["!pip install torch==2.1.0"],"id":"71c6f61a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb318692"},"outputs":[],"source":["#!pip install -q laserembeddings==1.1.2\n","!pip install -q ftfy"],"id":"fb318692"},{"cell_type":"code","execution_count":null,"metadata":{"id":"565dbd10"},"outputs":[],"source":["!pip install fastcache\n","# import fasttext\n","import ftfy\n","import html\n","#import laserembeddings\n","import sys\n","\n","from fastcache import clru_cache\n","#from laserembeddings import Laser\n","from typing import List, Union\n","from urllib.parse import unquote\n","from laser_encoders import LaserEncoderPipeline\n"],"id":"565dbd10"},{"cell_type":"markdown","metadata":{"id":"a512d5b5"},"source":["****LASER Encoders****\n","\n","laser_encoders is the official Python package for the Facebook LASER library. It provides a simple and convenient way to use LASER embeddings in Python. It allows you to calculate multilingual sentence embeddings using the LASER toolkit. These embeddings can be utilized for various natural language processing tasks, including document classification, bitext filtering, and mining.\n","\n","This encodes each of the strings as a LASER embedding (1024 dimentional vector)\n","\n","For users familiar with the earlier version of LASER, you might have encountered the laserembeddings package. This package primarily dealt with LASER-1 model embeddings.\n","\n","For the latest LASER-2,3 models, use the newly introduced laser_encoders package, which offers better performance and support for a wider range of languages.\n"],"id":"a512d5b5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-vO3k7MHs_U"},"outputs":[],"source":["# Initialize the LASER encoder pipeline\n","encoder = LaserEncoderPipeline(lang=\"igbo\")\n","\n","# Encode sentences into embeddings\n","embeddings = encoder.encode_sentences([\"nnọọ, kedu ka ị mere\"])\n","# If you want the output embeddings to be L2-normalized, set normalize_embeddings to True\n","normalized_embeddings = encoder.encode_sentences([\"nnọọ, kedu ka ị mere\"], normalize_embeddings=True)\n","\n","print(embeddings)\n","print('\\n')\n","print(normalized_embeddings)"],"id":"2-vO3k7MHs_U"},{"cell_type":"code","source":[],"metadata":{"id":"hJNQUSa69yqP"},"id":"hJNQUSa69yqP","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"505ab095"},"outputs":[],"source":["%%bash\n","# DOCS: https://github.com/facebookresearch/LASER/blob/master/install_models.sh\n","mkdir -p models/laser/\n","# for FILE in bilstm.eparl21.2018-11-19.pt eparl21.fcodes eparl21.fvocab bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\n","for FILE in bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\n","    wget -cq https://dl.fbaipublicfiles.com/laser/models/$FILE -O models/laser/$FILE\n","done"],"id":"505ab095"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d8d5f55e"},"outputs":[],"source":["!pip install config\n","#from src.utils.fasttest_model import language_detect\n","# from src.utils.punkt_tokenizer import punkt_tokenize_sentences\n","\n","config = {\n","    \"laser\": {\n","        \"base_dir\":  \"./models/laser\",\n","        \"bpe_codes\": \"./models/laser/93langs.fcodes\",\n","        \"bpe_vocab\": \"./models/laser/93langs.fvocab\",\n","        \"encoder\":   \"./models/laser/bilstm.93langs.2018-12-26.pt\",\n","    }\n","}"],"id":"d8d5f55e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfb1248a"},"outputs":[],"source":["def laser_encode(text: Union[str, List[str]], lang='autodetect', normalize=True) -> np.ndarray:\n","    \"\"\"\n","    Encodes a corpus of text using LASER\n","    :param text: Large block of text (will be tokenized), or list of pre-tokenized sentences\n","    :param lang: 2 digit language code (optional autodetect)\n","    :return:     embedding matrix\n","    \"\"\"\n","    encoder = LaserEncoderPipeline(lang=\"igbo\")\n","\n","    if isinstance(text, str):\n","        # sentences = punkt_tokenize_sentences(text, lang=lang)\n","        sentences = ['text']\n","\n","    else:\n","        sentences = list(text)\n","\n","    embedding = encoder.encode_sentences(sentences)\n","\n","    if normalize:\n","        embedding = encoder.encode_sentences(sentences, normalize_embeddings=True)\n","\n","    return embedding"],"id":"dfb1248a"},{"cell_type":"markdown","metadata":{"id":"VJMAlJfDUmAp"},"source":["# The most known classification models\n","\n","- SVC\n","- Decision Tree\n","- Boosting\n","- Bagging\n","- Random Forest\n","- Gradient Boosting\n","- Multiple layer perceprton (neural network)\n","- KNN\n","- Logistic regression\n"],"id":"VJMAlJfDUmAp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccSUe68QOaSz"},"outputs":[],"source":["pip install tensorflow-addons"],"id":"ccSUe68QOaSz"},{"cell_type":"code","execution_count":null,"metadata":{"id":"J47jhq5uWWDN"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","from collections import Counter\n","\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, KFold, cross_validate\n","\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","sns.set(style='white', context='notebook', palette='deep')\n"],"id":"J47jhq5uWWDN"},{"cell_type":"code","execution_count":null,"metadata":{"id":"526d7748"},"outputs":[],"source":["from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.metrics import Precision, Recall\n","\n","# lb = LabelBinarizer()\n","\n","train_ratio = 0.70\n","validation_ratio = 0.20\n","test_ratio = 0.10\n","\n","# train_test split: test 30%\n","X_train, X_test, y_train, y_test = train_test_split(df['full_text'], df['Related'], test_size=0.3, random_state=0)\n","\n","#test is 10% of the initial dataset, val 20%\n","X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n","\n","print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)\n"],"id":"526d7748"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aac8b81d"},"outputs":[],"source":["print('Training text lenght: ', len(X_train))\n","print('Validation text: ', len(X_val))\n","print('Testing text: ',len(X_test))"],"id":"aac8b81d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b4c5aea"},"outputs":[],"source":["X_train = laser_encode(X_train)\n","X_val   = laser_encode(X_val)\n","X_test  = laser_encode(X_test)"],"id":"9b4c5aea"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6a6a7ac3"},"outputs":[],"source":["print('X_train.shape', X_train.shape)\n","print('Y_train.shape', y_train.shape)"],"id":"6a6a7ac3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4335bed"},"outputs":[],"source":["X_train"],"id":"a4335bed"},{"cell_type":"code","execution_count":null,"metadata":{"id":"085e3002"},"outputs":[],"source":["y_train"],"id":"085e3002"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecaf36fd"},"outputs":[],"source":["print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)"],"id":"ecaf36fd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4d0ebc9a"},"outputs":[],"source":["max_len = 1024\n"],"id":"4d0ebc9a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6LFVFZOVjcd"},"outputs":[],"source":["kfold = KFold(n_splits=10, random_state=42)\n","\n"],"id":"j6LFVFZOVjcd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"akO62FxKT7r0"},"outputs":[],"source":[],"id":"akO62FxKT7r0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"36a8fadf"},"outputs":[],"source":["#Anastasia Model\n","\n","model = tf.keras.Sequential([\n","    tf.keras.Input(shape=(1024,)),\n","    tf.keras.layers.Dense(512, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(8, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),\n","])"],"id":"36a8fadf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SFe6PkCWkHj"},"outputs":[],"source":["metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n","\n","random_state = 42\n","classifiers = []\n","\n","classifiers.append(DecisionTreeClassifier(random_state=random_state))\n","classifiers.append(RandomForestClassifier(random_state=random_state))\n","classifiers.append(KNeighborsClassifier())\n","classifiers.append(LogisticRegression(random_state = random_state))\n","\n","classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n","classifiers.append(GradientBoostingClassifier(random_state=random_state))\n","classifiers.append(MLPClassifier(random_state=random_state))\n","#classifiers.append(model)\n","\n","\n","results = pd.DataFrame()\n","\n","for classifier in classifiers:\n","    print(classifier)\n","    scores = cross_validate(classifier, X_train, y = y_train, scoring=metrics, cv=kfold, n_jobs=1)\n","    print(scores)\n","    row = pd.DataFrame(scores)\n","    row = pd.DataFrame(row.mean(axis=0)).T\n","    row['Classifier'] = str(classifier)\n","\n","    results = pd.concat([results, row])\n","\n","results"],"id":"1SFe6PkCWkHj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGWQgXJva-sV"},"outputs":[],"source":[],"id":"FGWQgXJva-sV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6187698"},"outputs":[],"source":["epochs = 1000\n","batch_size = 32"],"id":"b6187698"},{"cell_type":"code","source":[],"metadata":{"id":"ChUjQrEIn0b2"},"id":"ChUjQrEIn0b2","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qnRPT5mTn0Yd"},"id":"qnRPT5mTn0Yd","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ESpkGu8Mn0VY"},"id":"ESpkGu8Mn0VY","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"v5_8PYf1n0SR"},"id":"v5_8PYf1n0SR","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q-yV6Bm1n0PY"},"id":"q-yV6Bm1n0PY","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iXtUdwJgn0Mo"},"id":"iXtUdwJgn0Mo","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"R3Hjg2hSn0JW"},"id":"R3Hjg2hSn0JW","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Jt2Fg29ln0GQ"},"id":"Jt2Fg29ln0GQ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fd0f146d"},"outputs":[],"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=15)\n"],"id":"fd0f146d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccd3aebf"},"outputs":[],"source":["history = model.fit(X_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_split=0.2,\n","                    verbose=1,\n","                    validation_data=([X_val, y_val]),\n","                    callbacks=callback)"],"id":"ccd3aebf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d468c9c7"},"outputs":[],"source":["model.evaluate(X_test, y_test, verbose =1)\n"],"id":"d468c9c7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"707860c2"},"outputs":[],"source":["print(history.history.keys())\n"],"id":"707860c2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"165bec5f"},"outputs":[],"source":["    print('Train Accuracy')\n","    model.evaluate(X_train, y_train)\n","\n","    print('Test Accuracy')\n","    model.evaluate(X_test, y_test)"],"id":"165bec5f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"068a90a9"},"outputs":[],"source":["history.history['binary_accuracy']"],"id":"068a90a9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1776ad62"},"outputs":[],"source":["model.save('/content/gdrive/MyDrive/Models/Experimental_RelevanceModel_laser2.h5')\n"],"id":"1776ad62"},{"cell_type":"code","execution_count":null,"metadata":{"id":"43b2ad1a"},"outputs":[],"source":["plt.plot(history.history['binary_accuracy'])\n","plt.plot(history.history['val_binary_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('binary_accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"id":"43b2ad1a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"58d4e7da"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n"],"id":"58d4e7da"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c23e1b07"},"outputs":[],"source":["prediction = model.predict(X_test)\n","# show the inputs and predicted outputs\n","#for i in range(len(X_test)):\n","print(\"X=%s, Predicted=%s\" % (X_test[0], prediction[0]))"],"id":"c23e1b07"},{"cell_type":"code","execution_count":null,"metadata":{"id":"48c60a54"},"outputs":[],"source":["for i in range(len(prediction)):\n","    if (prediction[i] < 0.5):\n","        prediction[i] = 0\n","    else:\n","        prediction[i] = 1"],"id":"48c60a54"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a7fa2a5e"},"outputs":[],"source":["matrix = confusion_matrix(y_test, prediction)\n","print(matrix)\n","#The diagonal entries are the accuracies of each class\n","matrix.diagonal()/matrix.sum(axis=1)"],"id":"a7fa2a5e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7304133d"},"outputs":[],"source":["# from mlxtend.evaluate import accuracy_score\n","\n","# std_acc = accuracy_score(y_test, prediction)\n","# bin_acc = accuracy_score(y_test, prediction, method='binary', pos_label=1)\n","# avg_acc = accuracy_score(y_test, prediction, method='average')\n","\n","# print(f'Standard accuracy: {std_acc*100:.2f}%')\n","# # print(f'Class 1 accuracy: {bin_acc*100:.2f}%')\n","# print(f'Average per-class accuracy: {avg_acc*100:.2f}%')"],"id":"7304133d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"06cc2a37"},"outputs":[],"source":["# # Evaluate the model on the test data using `evaluate`\n","# print(\"Evaluate on test data\")\n","# results = model.evaluate(X_test, y_test, batch_size=128)\n","# print(\"test loss, test acc:\", results)"],"id":"06cc2a37"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b50b5a51"},"outputs":[],"source":[],"id":"b50b5a51"},{"cell_type":"markdown","metadata":{"id":"31_tLjTzEa2R"},"source":["THE BALANCED MODEL"],"id":"31_tLjTzEa2R"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjI6JQw8Ea2Z"},"outputs":[],"source":["from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.metrics import Precision, Recall\n","\n","# lb = LabelBinarizer()\n","\n","train_ratio = 0.70\n","validation_ratio = 0.20\n","test_ratio = 0.10\n","\n","# train_test split: test 30%\n","X_train, X_test, y_train, y_test = train_test_split(df_balanced['full_text'], df_balanced['Related'], test_size=0.20, random_state=0)\n","\n","#test is 10% of the initial dataset, val 20%\n","X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n","\n","print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)\n"],"id":"YjI6JQw8Ea2Z"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9_RpZOTkEa2a"},"outputs":[],"source":["print('Training text lenght: ', len(X_train))\n","print('Validation text: ', len(X_val))\n","print('Testing text: ',len(X_test))"],"id":"9_RpZOTkEa2a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojphR6GtEa2a"},"outputs":[],"source":["X_train =laser_encode(X_train)\n","X_val = laser_encode(X_val)\n","X_test = laser_encode(X_test)"],"id":"ojphR6GtEa2a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wuJmnafoEa2a"},"outputs":[],"source":["print('X_train.shape', X_train.shape)\n","print('Y_train.shape', y_train.shape)"],"id":"wuJmnafoEa2a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ri1yQw6lEa2a"},"outputs":[],"source":["X_train"],"id":"ri1yQw6lEa2a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPFI_TLBEa2b"},"outputs":[],"source":["y_train"],"id":"HPFI_TLBEa2b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"FRawKUcYEa2b"},"outputs":[],"source":["print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)"],"id":"FRawKUcYEa2b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-hfbP-VEa2b"},"outputs":[],"source":["max_len = 1024\n"],"id":"C-hfbP-VEa2b"},{"cell_type":"markdown","metadata":{"id":"Ii9G7gkXEa2b"},"source":["> Neural Network - TF Keras"],"id":"Ii9G7gkXEa2b"},{"cell_type":"markdown","metadata":{"id":"BYX5PB4JEa2b"},"source":["This inputs a 1024 LASER embedding and outputs a 1 bit classification prediction.\n","\n","A triangular shaped architecture is used, including Dropout and BatchNorm.\n","\n","2022/07/06 Updated LASER models with support for over 200 languages"],"id":"BYX5PB4JEa2b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSY6RqLlEa2c"},"outputs":[],"source":["pip install tensorflow-addons"],"id":"mSY6RqLlEa2c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"akfTAkgVEa2c"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_addons as tfa\n"],"id":"akfTAkgVEa2c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKfPhiYwEa2c"},"outputs":[],"source":["# Build the model\n","\n","accuracy_metric = tf.keras.metrics.BinaryAccuracy()\n","precision_metric = Precision()\n","recall_metric = Recall()\n","f1_score_metric = tfa.metrics.F1Score(num_classes=1, threshold=0.5)\n","\n","model = tf.keras.Sequential([\n","    tf.keras.Input(shape=(1024,)),\n","    tf.keras.layers.Dense(512, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(8, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dropout(0.25),\n","    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),\n","])\n","model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n","              metrics =  [ accuracy_metric, precision_metric, recall_metric, f1_score_metric ])\n","print(model.summary())"],"id":"fKfPhiYwEa2c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vpLfhk6Ea2c"},"outputs":[],"source":["epochs = 1000\n","batch_size = 32"],"id":"-vpLfhk6Ea2c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRwki4rrEa2d"},"outputs":[],"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=7)\n"],"id":"rRwki4rrEa2d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPfFbFZmEa2d"},"outputs":[],"source":["history = model.fit(X_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_split=0.2,\n","                    verbose=1,\n","                    validation_data=([X_val, y_val]),\n","                    callbacks=callback)"],"id":"kPfFbFZmEa2d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsKyWhZREa2d"},"outputs":[],"source":["model.evaluate(X_test, y_test, verbose =1)\n"],"id":"qsKyWhZREa2d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"15sViuWhEa2e"},"outputs":[],"source":["print(history.history.keys())\n"],"id":"15sViuWhEa2e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JK7Qes91Ea2e"},"outputs":[],"source":["    print('Train Accuracy')\n","    model.evaluate(X_train, y_train)\n","\n","    print('Test Accuracy')\n","    model.evaluate(X_test, y_test)"],"id":"JK7Qes91Ea2e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wMo5_AtHEa2f"},"outputs":[],"source":["history.history['binary_accuracy']"],"id":"wMo5_AtHEa2f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jbT5ASLEa2g"},"outputs":[],"source":["model.save('/content/gdrive/MyDrive/Models/Max_BalancedRelevanceModel_laser2.h5')\n"],"id":"4jbT5ASLEa2g"},{"cell_type":"code","execution_count":null,"metadata":{"id":"778XMT6DEa2g"},"outputs":[],"source":["plt.plot(history.history['binary_accuracy'])\n","plt.plot(history.history['val_binary_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('binary_accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"id":"778XMT6DEa2g"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5aEpqiR_Ea2h"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n"],"id":"5aEpqiR_Ea2h"},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzTWNeWoEa2i"},"outputs":[],"source":["prediction = model.predict(X_test)\n","# show the inputs and predicted outputs\n","#for i in range(len(X_test)):\n","print(\"X=%s, Predicted=%s\" % (X_test[0], prediction[0]))"],"id":"wzTWNeWoEa2i"},{"cell_type":"code","execution_count":null,"metadata":{"id":"tpU-XRP3Ea2i"},"outputs":[],"source":["for i in range(len(prediction)):\n","    if (prediction[i] < 0.5):\n","        prediction[i] = 0\n","    else:\n","        prediction[i] = 1"],"id":"tpU-XRP3Ea2i"},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5e0zgmWEa2j"},"outputs":[],"source":["matrix = confusion_matrix(y_test, prediction)\n","print(matrix)\n","#The diagonal entries are the accuracies of each class\n","matrix.diagonal()/matrix.sum(axis=1)"],"id":"i5e0zgmWEa2j"},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_lMoBOnprv9"},"outputs":[],"source":[],"id":"D_lMoBOnprv9"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"papermill":{"default_parameters":{},"duration":418.715336,"end_time":"2022-12-04T15:05:21.058802","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-12-04T14:58:22.343466","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}